{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel environment where the agent is located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, the environment and environment in which the agent is located are mainly defined as the reward of the action selected by the agent. In class Env, the following variables and functions are defined:\n",
    "variable:\n",
    "* _actions_: indicates the optional actions of the agent, using the list to represent\n",
    "* _n_actions_: record the number of optional actions, used in the neural network output in subsequent DQN\n",
    "* _n_features_: The number of attributes for recording observations, 4 in this project\n",
    "* _time_env_state_: use a dictionary to record a specific moment, the state of the environment\n",
    "function:\n",
    "* \\_\\__init_\\_\\_(_self_): initialize the function, define the variable\n",
    "* _update_\\__State_(_self_): get the current environment information from the environment data, and return\n",
    "* _reset_(_self_): reset the environment, (this can be removed)\n",
    "* _step_(_self_, _action_): according to the action selected by the agent, return the status value and reward of the next moment of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env( ):\n",
    "    def __init__(self):\n",
    "        super(Env, self).__init__()\n",
    "        self.actions = [\n",
    "            \"Channel_1\",\n",
    "            \"Channel_6\",\n",
    "            \"Channel_11\"\n",
    "        ]\n",
    "\n",
    "        self.tx_power_list = [20, 24, 27, 30]\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.n_features = 4\n",
    "        self.state =\"\"\n",
    "        self.time = 1\n",
    "        self.count = 1\n",
    "        self.count_history = []\n",
    "        self.time_env_state = {}\n",
    "\n",
    "    def data(self):\n",
    "        RSSI_1 = random.randint(-100, 0)\n",
    "        RSSI_2 = random.randint(-100, 0)\n",
    "        tx_power = random.choice(self.tx_power_list)\n",
    "        spectral_density = random.uniform(0, 100)\n",
    "        channel = np.array([RSSI_1, RSSI_2, tx_power, spectral_density])\n",
    "        return channel\n",
    "    \n",
    "    def update_State(self):\n",
    "        self.time_env_state[\"current\"] = {\"Channel_1\": self.data(),\n",
    "                                         \"Channel_6\": self.data(),\n",
    "                                         \"Channel_11\": self.data(),}\n",
    "\n",
    "        return self.time_env_state[\"current\"]\n",
    "\n",
    "    def reset(self):\n",
    "        self.update_State()\n",
    "        self.state = \"Channel_1\"\n",
    "        return self.time_env_state[\"current\"][self.state]\n",
    "\n",
    "    def value(self, state):\n",
    "        # state[:2] = [state[0], state[1]] = [RSSI_1, RSSI_2]\n",
    "        # state[2] = tx_power\n",
    "        # state[3] = spectral_density\n",
    "        RSSI_1 = state[0]\n",
    "        RSSI_2 = state[1]\n",
    "        tx_power = state[2]\n",
    "        spectral_density = state[3]\n",
    "        \n",
    "        # Value returned is -power_sum([RSSI_1, RSSI_2]) + tx_power + (100 - spectral_density)\n",
    "        # Higher, more positive is better, hence abs() for power sum of RSSIs.\n",
    "        return np.abs(10*np.log10(np.sum(10**(np.array([RSSI_1, RSSI_2])/10)))) + tx_power + (100 - spectral_density)\n",
    "        # pass\n",
    "\n",
    "    def step(self, action):\n",
    "        self.time += 1\n",
    "        self.update_State()\n",
    "        value = 0\n",
    "        max_value = -np.Inf # min. possible value\n",
    "        action_key = \"\"\n",
    "        # \"\"\"\n",
    "        # Find value function output for each chan. in current state.\n",
    "        for key in self.time_env_state[\"current\"]:\n",
    "\n",
    "            value = self.value(self.time_env_state[\"current\"][key])\n",
    "            \n",
    "            # 比较 - compare & update current max. value, return action for max. value\n",
    "            if value > max_value:\n",
    "                max_value = value\n",
    "                action_key = key\n",
    "        # \"\"\"\n",
    "        \n",
    "        print('{}: action {} has max. value {}\\n'.format(self.time, action_key, max_value))\n",
    "        \n",
    "        # print(self.time)\n",
    "        next_state = action\n",
    "\n",
    "        if next_state == action_key:\n",
    "            self.count += 1\n",
    "            reward = self.count\n",
    "            print('{}: selected action = action with max. value, reward = {}\\n'.format(self.time, action, reward))\n",
    "            \n",
    "        else:\n",
    "            reward = 0\n",
    "            self.count = 1\n",
    "            \n",
    "        self.count_history.append(self.count)\n",
    "            \n",
    "        self.state = next_state\n",
    "        return self.time_env_state[\"current\"][next_state], reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
